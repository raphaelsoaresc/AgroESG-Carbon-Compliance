# üåø AgroESG Carbon Compliance

> **Status:** üöÄ Ingest√£o (EL) Operacional | üèóÔ∏è Transforma√ß√£o (dbt) em Desenvolvimento

Este projeto √© uma solu√ß√£o de **Engenharia de Dados (ELT)** focada em an√°lise de risco e compliance ambiental para a origina√ß√£o de cr√©ditos de carbono. O objetivo √© cruzar dados geoespaciais de propriedades rurais (SIGEF) com listas de embargos ambientais (IBAMA), garantindo a elegibilidade ESG atrav√©s de uma arquitetura resiliente e idempotente.

## üéØ O Problema de Neg√≥cio

Para emitir cr√©ditos de carbono de alta integridade, √© necess√°rio garantir que a √°rea do projeto n√£o possui sobreposi√ß√£o com √°reas embargadas. O desafio reside na instabilidade das fontes governamentais e na complexidade dos dados geoespaciais. Este projeto implementa um pipeline que garante a **rastreabilidade hist√≥rica** e a **unicidade dos dados**, mesmo em casos de reprocessamento.

## üèó Arquitetura e Stack

O projeto utiliza uma abordagem **Medallion Architecture** (Bronze, Silver, Gold) orquestrada por um ambiente imut√°vel via Nix.

graph TD
    subgraph "Ingest√£o & Pr√©-Processamento (Local/DuckDB)"
        A[Arquivos Brutos: SIGEF/IBAMA] --> B[DuckDB Spatial]
        B -->|Hash MD5 & Parquet| C[Local Staging]
    end
    
    subgraph "Cloud Storage (Bronze Layer)"
        C -->|Upload Idempotente| D[Google Cloud Storage]
        D -->|Write Append| E[BigQuery Raw Tables]
    end

    subgraph "Transforma√ß√£o (Cloud/dbt)"
        E --> F[dbt Core: Silver Layer]
        F -->|Deduplica√ß√£o & Spatial Join| G[BigQuery Gold: Compliance]
    end


### üõ†Ô∏è Destaques de Engenharia de Dados

*   **Idempot√™ncia Garantida:** Implementa√ß√£o de Hashing MD5 para cada arquivo processado. O pipeline utiliza nomes determin√≠sticos no GCS para evitar lixo no storage e metadados de auditoria (`file_hash`, `ingested_at`) no BigQuery.
*   **Processamento Espacial de Alta Performance:** Uso do **DuckDB** para leitura de Shapefiles complexos e convers√£o local para Parquet. A geometria √© tratada via `ST_AsText` para garantir compatibilidade total com o BigQuery.
*   **Resili√™ncia no Airflow 2.10:** Supera√ß√£o de bugs de serializa√ß√£o de metadados (JSON/Pickle) atrav√©s da implementa√ß√£o do `BigQueryInsertJobOperator`, garantindo uma comunica√ß√£o robusta com a API de Jobs do Google Cloud.
*   **Estrat√©gia de Housekeeping:** Sistema autom√°tico de arquivamento de arquivos processados, prevenindo reprocessamentos infinitos e garantindo a limpeza do ambiente local.

## üß∞ Stack T√©cnica

*   **Orquestra√ß√£o:** Apache Airflow 2.10 (rodando com Postgres Backend e LocalExecutor).
*   **Motor de Dados:** DuckDB (com extens√µes Spatial e HTTPFS).
*   **Data Warehouse:** Google BigQuery & Cloud Storage.
*   **Transforma√ß√£o:** dbt Core (em implementa√ß√£o).
*   **Infraestrutura:** `devenv` (Nix) e `uv` para ambientes 100% reprodut√≠veis.

## üöÄ Como Executar o Projeto

Este projeto utiliza **Nix**. N√£o √© necess√°rio instalar Python, Postgres ou depend√™ncias manualmente.

### Passo a Passo

1.  **Entre no shell de desenvolvimento:**
    ```bash
    devenv shell
    ```
    *Isso prepara o ambiente isolado com Python, `uv`, Postgres e as depend√™ncias do Airflow.*

2.  **Ative os servi√ßos de infraestrutura:**
    ```bash
    devenv up -d
    ```
    *Inicia o banco Postgres em segundo plano (essencial para o Metastore do Airflow).*

3.  **Inicie o Orquestrador:**
    ```bash
    start-airflow
    ```
    *Acesse `localhost:8080`. Credenciais padr√£o: `admin` / `admin`.*

4.  **Configura√ß√£o de Conex√µes (Airflow UI):**
    *   **`fs_default`**: Tipo `File (path)`, aponte o `Extra` ou `Path` para a raiz do seu diret√≥rio de dados local.
    *   **`google_cloud_default`**: Tipo `Google Cloud`, insira o JSON da sua Service Account para permiss√µes no BigQuery e GCS.


## üó∫ Roadmap

* [x] **Infraestrutura:** Ambiente Nix com Postgres e Airflow configurados.
* [x] **Ingest√£o SIGEF:** DAG idempotente com DuckDB Spatial e carga no BigQuery.
* [x] **Ingest√£o IBAMA:** DAG resiliente com suporte a CSV/Shapefile e carga via BigQuery Jobs.
* [ ] **Camada Silver (dbt):** Modelos de limpeza e deduplica√ß√£o l√≥gica (Last Record Wins).
* [ ] **Camada Gold (dbt):** Implementa√ß√£o do Spatial Join para detec√ß√£o de sobreposi√ß√µes.
* [ ] **Dashboard:** Visualiza√ß√£o de risco ESG no Looker Studio.

---
**Autor:** Raphael Soares

*Projeto desenvolvido para portf√≥lio de Data Engineering & Analytics.*